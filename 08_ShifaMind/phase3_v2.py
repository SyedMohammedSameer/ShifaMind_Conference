#!/usr/bin/env python3
"""
================================================================================
SHIFAMIND PHASE 3 V2: RAG + Citation Head
================================================================================
Author: Mohammed Sameer Syed
University of Arizona - MS in AI Capstone

This phase adds:
1. Retrieval-Augmented Generation (RAG) for evidence grounding
2. Citation Head for evidence attribution
3. Multi-head outputs: Diagnosis, Citation, Action
4. Evidence-based explainability

Architecture:
- Load Phase 2 checkpoint (concept bottleneck + GraphSAGE)
- Build evidence retriever using dense retrieval (DPR)
- Add Citation Head to predict which evidence supports diagnosis
- Add Action Head for clinical recommendations
- Multi-objective loss: L_dx + L_align + L_concept + L_cite + L_action

Target Metrics:
- Diagnosis F1: >0.75
- Concept F1: >0.75
- Citation Precision@3: >0.70 (top 3 evidence passages are relevant)
- Evidence grounding score: >0.80

Saves:
- Enhanced model with citation + action heads
- Evidence database with embeddings
- Citation mappings

================================================================================
"""

print("="*80)
print("ðŸš€ SHIFAMIND PHASE 3 V2 - RAG + CITATION HEAD")
print("="*80)

# ============================================================================
# IMPORTS & SETUP
# ============================================================================

import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torch_geometric
from torch_geometric.nn import SAGEConv

import numpy as np
import pandas as pd
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score
from transformers import (
    AutoTokenizer, AutoModel,
    get_linear_schedule_with_warmup
)

import json
import pickle
from pathlib import Path
from tqdm.auto import tqdm
from typing import Dict, List, Tuple, Set
from collections import defaultdict
import re

# Reproducibility
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nðŸ–¥ï¸  Device: {device}")

# ============================================================================
# CONFIGURATION
# ============================================================================

print("\n" + "="*80)
print("âš™ï¸  CONFIGURATION")
print("="*80)

# Local environment path
BASE_PATH = Path('/home/user/ShifaMind_Conference')
OUTPUT_BASE = BASE_PATH / '08_ShifaMind'

# Paths
PHASE2_CHECKPOINT = OUTPUT_BASE / 'checkpoints/phase2_v2/phase2_v2_best.pt'
CHECKPOINT_PATH = OUTPUT_BASE / 'checkpoints/phase3_v2'
SHARED_DATA_PATH = OUTPUT_BASE / 'shared_data'
RESULTS_PATH = OUTPUT_BASE / 'results/phase3_v2'
CONCEPT_STORE_PATH = OUTPUT_BASE / 'concept_store'
EVIDENCE_PATH = OUTPUT_BASE / 'evidence_store'

# Create directories
for path in [CHECKPOINT_PATH, RESULTS_PATH, EVIDENCE_PATH]:
    path.mkdir(parents=True, exist_ok=True)

print(f"ðŸ“ Phase 2 Checkpoint: {PHASE2_CHECKPOINT}")
print(f"ðŸ“ Checkpoints: {CHECKPOINT_PATH}")
print(f"ðŸ“ Shared Data: {SHARED_DATA_PATH}")
print(f"ðŸ“ Results: {RESULTS_PATH}")
print(f"ðŸ“ Evidence Store: {EVIDENCE_PATH}")

# Target diagnoses (ICD-10 codes)
TARGET_CODES = ['J189', 'I5023', 'A419', 'K8000']
ICD_DESCRIPTIONS = {
    'J189': 'Pneumonia, unspecified organism',
    'I5023': 'Acute on chronic systolic heart failure',
    'A419': 'Sepsis, unspecified organism',
    'K8000': 'Calculus of gallbladder with acute cholecystitis'
}

# Clinical actions
CLINICAL_ACTIONS = [
    'Order chest X-ray',
    'Order blood cultures',
    'Administer oxygen therapy',
    'Initiate antibiotic therapy',
    'Order echocardiogram',
    'Initiate diuretic therapy',
    'Consult surgery',
    'Order abdominal ultrasound',
    'Admit to ICU',
    'Monitor vitals closely'
]

# Load concept list from Phase 1
with open(SHARED_DATA_PATH / 'concept_list.json', 'r') as f:
    ALL_CONCEPTS = json.load(f)

print(f"\nðŸŽ¯ Target: {len(TARGET_CODES)} diagnoses")
print(f"ðŸ§  Concepts: {len(ALL_CONCEPTS)} clinical concepts")
print(f"ðŸ’¡ Actions: {len(CLINICAL_ACTIONS)} clinical actions")

# RAG hyperparameters
MAX_EVIDENCE_PASSAGES = 5
EVIDENCE_EMBEDDING_DIM = 768

# Training hyperparameters
LAMBDA_DX = 1.0
LAMBDA_ALIGN = 0.5
LAMBDA_CONCEPT = 0.3
LAMBDA_CITE = 0.3  # NEW: Citation loss
LAMBDA_ACTION = 0.2  # NEW: Action recommendation loss
LEARNING_RATE = 5e-6  # Even lower for fine-tuning
EPOCHS = 3

print(f"\nâš–ï¸  Loss Weights:")
print(f"   Î»_dx:      {LAMBDA_DX}")
print(f"   Î»_align:   {LAMBDA_ALIGN}")
print(f"   Î»_concept: {LAMBDA_CONCEPT}")
print(f"   Î»_cite:    {LAMBDA_CITE} â† NEW: Citation loss")
print(f"   Î»_action:  {LAMBDA_ACTION} â† NEW: Action loss")

# ============================================================================
# BUILD EVIDENCE DATABASE
# ============================================================================

print("\n" + "="*80)
print("ðŸ“š BUILDING EVIDENCE DATABASE")
print("="*80)

def build_evidence_database():
    """
    Build evidence database with clinical knowledge

    In production, this would load from:
    - PubMed articles
    - Clinical guidelines
    - UpToDate summaries
    - MIMIC-CXR reports

    For now, creating knowledge-based evidence passages
    """
    print("\nðŸ“– Creating evidence database...")

    evidence_db = []

    # Evidence for J189 (Pneumonia)
    evidence_db.extend([
        {
            'passage_id': 'J189_001',
            'text': 'Pneumonia is typically diagnosed based on clinical presentation (fever, cough, dyspnea) combined with radiographic evidence of infiltrates on chest X-ray. Consolidation patterns are highly specific for bacterial pneumonia.',
            'diagnosis': 'J189',
            'evidence_type': 'diagnostic_criteria',
            'source': 'Clinical Practice Guideline'
        },
        {
            'passage_id': 'J189_002',
            'text': 'Bronchial breath sounds and dullness to percussion are classic physical exam findings in pneumonia, indicating lung consolidation. Fever and productive cough with purulent sputum support bacterial etiology.',
            'diagnosis': 'J189',
            'evidence_type': 'clinical_signs',
            'source': 'Physical Examination Guide'
        },
        {
            'passage_id': 'J189_003',
            'text': 'Chest imaging showing infiltrates or consolidation is essential for pneumonia diagnosis. Lower lobe infiltrates are common. Respiratory rate elevation and hypoxia indicate severity.',
            'diagnosis': 'J189',
            'evidence_type': 'imaging',
            'source': 'Radiology Reference'
        }
    ])

    # Evidence for I5023 (Heart Failure)
    evidence_db.extend([
        {
            'passage_id': 'I5023_001',
            'text': 'Acute on chronic systolic heart failure presents with dyspnea, orthopnea, and paroxysmal nocturnal dyspnea. Elevated BNP levels (>400 pg/mL) strongly support diagnosis. Reduced ejection fraction on echocardiogram confirms systolic dysfunction.',
            'diagnosis': 'I5023',
            'evidence_type': 'diagnostic_criteria',
            'source': 'Cardiology Guidelines'
        },
        {
            'passage_id': 'I5023_002',
            'text': 'Physical examination findings in heart failure include bilateral lower extremity edema, elevated jugular venous pressure, and S3 gallop on cardiac auscultation. Cardiomegaly may be visible on chest X-ray.',
            'diagnosis': 'I5023',
            'evidence_type': 'clinical_signs',
            'source': 'Physical Examination Guide'
        },
        {
            'passage_id': 'I5023_003',
            'text': 'Pulmonary edema on chest imaging combined with cardiac dysfunction on echocardiography confirms heart failure. BNP biomarker testing aids in diagnosis and risk stratification.',
            'diagnosis': 'I5023',
            'evidence_type': 'lab_imaging',
            'source': 'Diagnostic Reference'
        }
    ])

    # Evidence for A419 (Sepsis)
    evidence_db.extend([
        {
            'passage_id': 'A419_001',
            'text': 'Sepsis is defined by life-threatening organ dysfunction caused by dysregulated host response to infection. Clinical criteria include fever/hypothermia, tachycardia, hypotension, and altered mental status. Lactate elevation >2 mmol/L indicates tissue hypoperfusion.',
            'diagnosis': 'A419',
            'evidence_type': 'diagnostic_criteria',
            'source': 'Sepsis-3 Criteria'
        },
        {
            'passage_id': 'A419_002',
            'text': 'Septic shock requires hypotension despite fluid resuscitation and lactate >2 mmol/L. Blood cultures should be obtained before antibiotic administration. WBC elevation with left shift indicates bacterial infection.',
            'diagnosis': 'A419',
            'evidence_type': 'clinical_signs',
            'source': 'Critical Care Guidelines'
        },
        {
            'passage_id': 'A419_003',
            'text': 'Sepsis diagnosis requires evidence of infection plus organ dysfunction. Hemodynamic instability and vasopressor requirement indicate septic shock. Early antibiotic administration improves outcomes.',
            'diagnosis': 'A419',
            'evidence_type': 'treatment',
            'source': 'Surviving Sepsis Campaign'
        }
    ])

    # Evidence for K8000 (Acute Cholecystitis)
    evidence_db.extend([
        {
            'passage_id': 'K8000_001',
            'text': 'Acute cholecystitis with cholelithiasis presents with right upper quadrant pain, fever, and positive Murphy sign. Ultrasound shows gallbladder wall thickening (>3mm) and pericholecystic fluid.',
            'diagnosis': 'K8000',
            'evidence_type': 'diagnostic_criteria',
            'source': 'Tokyo Guidelines'
        },
        {
            'passage_id': 'K8000_002',
            'text': 'Murphy sign (inspiratory arrest during RUQ palpation) is highly specific for acute cholecystitis. Pain may radiate to right shoulder. Leukocytosis and elevated inflammatory markers support diagnosis.',
            'diagnosis': 'K8000',
            'evidence_type': 'clinical_signs',
            'source': 'Surgical Reference'
        },
        {
            'passage_id': 'K8000_003',
            'text': 'Ultrasound is first-line imaging for suspected cholecystitis, showing gallstones and gallbladder inflammation. Cholestasis with elevated bilirubin may indicate common bile duct involvement.',
            'diagnosis': 'K8000',
            'evidence_type': 'imaging',
            'source': 'Radiology Guidelines'
        }
    ])

    # General evidence
    evidence_db.extend([
        {
            'passage_id': 'GEN_001',
            'text': 'Fever is a common presenting symptom in infectious and inflammatory conditions. Temperature >38.3Â°C (101Â°F) is clinically significant. Fever patterns can help differentiate etiologies.',
            'diagnosis': 'general',
            'evidence_type': 'vital_signs',
            'source': 'Clinical Medicine Textbook'
        },
        {
            'passage_id': 'GEN_002',
            'text': 'Dyspnea (shortness of breath) can result from cardiac, pulmonary, or systemic causes. Assessment includes respiratory rate, oxygen saturation, and effort of breathing.',
            'diagnosis': 'general',
            'evidence_type': 'symptoms',
            'source': 'Clinical Medicine Textbook'
        }
    ])

    print(f"âœ… Evidence database created:")
    print(f"   Total passages: {len(evidence_db)}")
    print(f"   J189 (Pneumonia): {len([e for e in evidence_db if e['diagnosis'] == 'J189'])}")
    print(f"   I5023 (Heart Failure): {len([e for e in evidence_db if e['diagnosis'] == 'I5023'])}")
    print(f"   A419 (Sepsis): {len([e for e in evidence_db if e['diagnosis'] == 'A419'])}")
    print(f"   K8000 (Cholecystitis): {len([e for e in evidence_db if e['diagnosis'] == 'K8000'])}")
    print(f"   General: {len([e for e in evidence_db if e['diagnosis'] == 'general'])}")

    return evidence_db

evidence_database = build_evidence_database()

# Save evidence database
with open(EVIDENCE_PATH / 'evidence_database.json', 'w') as f:
    json.dump(evidence_database, f, indent=2)

# ============================================================================
# EVIDENCE RETRIEVER
# ============================================================================

print("\n" + "="*80)
print("ðŸ” BUILDING EVIDENCE RETRIEVER")
print("="*80)

class EvidenceRetriever(nn.Module):
    """
    Dense retrieval for evidence passages

    Uses BioClinicalBERT to encode both queries (clinical notes)
    and evidence passages into dense vectors for similarity matching
    """
    def __init__(self, encoder_model):
        super().__init__()
        self.encoder = encoder_model

    def encode_text(self, input_ids, attention_mask):
        """Encode text to dense vector"""
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        # Use [CLS] token embedding
        return outputs.last_hidden_state[:, 0, :]  # [batch, hidden_size]

    def retrieve(self, query_embedding, evidence_embeddings, top_k=5):
        """
        Retrieve top-k most similar evidence passages

        Args:
            query_embedding: [batch, hidden_size]
            evidence_embeddings: [num_passages, hidden_size]
            top_k: Number of passages to retrieve

        Returns:
            indices: [batch, top_k] - indices of top passages
            scores: [batch, top_k] - similarity scores
        """
        # Compute cosine similarity
        query_norm = F.normalize(query_embedding, p=2, dim=-1)
        evidence_norm = F.normalize(evidence_embeddings, p=2, dim=-1)

        similarities = torch.matmul(query_norm, evidence_norm.t())  # [batch, num_passages]

        # Get top-k
        scores, indices = torch.topk(similarities, k=top_k, dim=-1)

        return indices, scores

# Create retriever
retriever_tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')
retriever_encoder = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT').to(device)
retriever = EvidenceRetriever(retriever_encoder).to(device)

# Encode all evidence passages
print("\nðŸ”¢ Encoding evidence passages...")
evidence_texts = [e['text'] for e in evidence_database]
evidence_encodings = retriever_tokenizer(
    evidence_texts,
    truncation=True,
    max_length=256,
    padding='max_length',
    return_tensors='pt'
)

with torch.no_grad():
    retriever.eval()
    evidence_embeddings = retriever.encode_text(
        evidence_encodings['input_ids'].to(device),
        evidence_encodings['attention_mask'].to(device)
    )  # [num_passages, 768]

print(f"âœ… Evidence embeddings created: {evidence_embeddings.shape}")

# Save evidence embeddings
torch.save(evidence_embeddings, EVIDENCE_PATH / 'evidence_embeddings.pt')

# ============================================================================
# SHIFAMIND PHASE 3 MODEL
# ============================================================================

print("\n" + "="*80)
print("ðŸ—ï¸  BUILDING SHIFAMIND PHASE 3 MODEL")
print("="*80)

class ShifaMindPhase3(nn.Module):
    """
    ShifaMind with RAG + Citation + Action Heads

    Architecture:
    1. BioClinicalBERT encoder (from Phase 1)
    2. GraphSAGE encoder for ontology (from Phase 2)
    3. Concept bottleneck with cross-attention
    4. Evidence retrieval and grounding
    5. Multi-head outputs:
       - Diagnosis Head
       - Citation Head (which evidence supports diagnosis)
       - Action Head (recommended clinical actions)
    """
    def __init__(self, base_model, retriever, evidence_embeddings, num_concepts, num_diagnoses, num_actions, hidden_size=768):
        super().__init__()

        self.bert = base_model
        self.retriever = retriever
        self.hidden_size = hidden_size
        self.num_concepts = num_concepts
        self.num_diagnoses = num_diagnoses
        self.num_actions = num_actions

        # Register evidence embeddings as buffer
        self.register_buffer('evidence_embeddings', evidence_embeddings)

        # Evidence attention (integrate retrieved evidence)
        self.evidence_attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )

        # Concept bottleneck (from Phase 1 & 2)
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )

        self.gate_net = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Sigmoid()
        )

        self.layer_norm = nn.LayerNorm(hidden_size)

        # Output heads
        self.concept_head = nn.Linear(hidden_size, num_concepts)
        self.diagnosis_head = nn.Linear(hidden_size, num_diagnoses)
        self.citation_head = nn.Linear(hidden_size, MAX_EVIDENCE_PASSAGES)  # Which evidence supports diagnosis
        self.action_head = nn.Linear(hidden_size, num_actions)  # Recommended actions

    def forward(self, input_ids, attention_mask, concept_embeddings):
        """
        Forward pass with evidence grounding

        Args:
            input_ids: Tokenized text [batch, seq_len]
            attention_mask: Attention mask [batch, seq_len]
            concept_embeddings: Concept embeddings [num_concepts, hidden_size]

        Returns:
            Dictionary with all outputs
        """
        batch_size = input_ids.shape[0]

        # 1. Encode text with BERT
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state  # [batch, seq_len, hidden_size]

        # 2. Retrieve evidence passages
        query_embedding = hidden_states[:, 0, :]  # [CLS] token: [batch, hidden_size]
        evidence_indices, evidence_scores = self.retriever.retrieve(
            query_embedding,
            self.evidence_embeddings,
            top_k=MAX_EVIDENCE_PASSAGES
        )  # [batch, top_k]

        # 3. Get retrieved evidence embeddings
        retrieved_evidence = self.evidence_embeddings[evidence_indices]  # [batch, top_k, hidden_size]

        # 4. Evidence attention: integrate evidence with text representation
        evidence_context, evidence_attn = self.evidence_attention(
            query=hidden_states,
            key=retrieved_evidence,
            value=retrieved_evidence,
            need_weights=True
        )  # [batch, seq_len, hidden_size]

        # 5. Concept bottleneck
        bert_concepts = concept_embeddings.unsqueeze(0).expand(batch_size, -1, -1)
        concept_context, concept_attn = self.cross_attention(
            query=evidence_context,  # Use evidence-enhanced representation
            key=bert_concepts,
            value=bert_concepts,
            need_weights=True
        )

        # 6. Multiplicative bottleneck gating
        pooled_text = evidence_context.mean(dim=1)
        pooled_context = concept_context.mean(dim=1)

        gate_input = torch.cat([pooled_text, pooled_context], dim=-1)
        gate = self.gate_net(gate_input)

        bottleneck_output = gate * pooled_context
        bottleneck_output = self.layer_norm(bottleneck_output)

        # 7. Multi-head outputs
        concept_logits = self.concept_head(pooled_text)
        diagnosis_logits = self.diagnosis_head(bottleneck_output)
        citation_logits = self.citation_head(bottleneck_output)  # Which evidence passages are relevant
        action_logits = self.action_head(bottleneck_output)  # Recommended clinical actions

        return {
            'logits': diagnosis_logits,
            'concept_logits': concept_logits,
            'citation_logits': citation_logits,
            'action_logits': action_logits,
            'concept_scores': torch.sigmoid(concept_logits),
            'citation_scores': torch.sigmoid(citation_logits),
            'action_scores': torch.sigmoid(action_logits),
            'gate_values': gate,
            'evidence_indices': evidence_indices,
            'evidence_scores': evidence_scores,
            'evidence_attention': evidence_attn,
            'concept_attention': concept_attn
        }

# Initialize model
print("\nðŸ”§ Initializing model components...")
tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')
base_model = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT').to(device)
concept_embedding_layer = nn.Embedding(len(ALL_CONCEPTS), 768).to(device)

model = ShifaMindPhase3(
    base_model=base_model,
    retriever=retriever,
    evidence_embeddings=evidence_embeddings,
    num_concepts=len(ALL_CONCEPTS),
    num_diagnoses=len(TARGET_CODES),
    num_actions=len(CLINICAL_ACTIONS),
    hidden_size=768
).to(device)

# Load Phase 2 weights if available
if PHASE2_CHECKPOINT.exists():
    print(f"\nðŸ“¥ Loading Phase 2 checkpoint...")
    checkpoint = torch.load(PHASE2_CHECKPOINT, map_location=device, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    print("âœ… Loaded Phase 2 weights (partial)")
else:
    print("âš ï¸  Phase 2 checkpoint not found - training from scratch")

print(f"\nâœ… ShifaMind Phase 3 model initialized")
print(f"   Total parameters: {sum(p.numel() for p in model.parameters()):,}")

# ============================================================================
# TRAINING SETUP
# ============================================================================

print("\n" + "="*80)
print("âš™ï¸  TRAINING SETUP")
print("="*80)

# Load data
with open(SHARED_DATA_PATH / 'train_split.pkl', 'rb') as f:
    df_train = pickle.load(f)
with open(SHARED_DATA_PATH / 'val_split.pkl', 'rb') as f:
    df_val = pickle.load(f)
with open(SHARED_DATA_PATH / 'test_split.pkl', 'rb') as f:
    df_test = pickle.load(f)

train_concept_labels = np.load(SHARED_DATA_PATH / 'train_concept_labels.npy')
val_concept_labels = np.load(SHARED_DATA_PATH / 'val_concept_labels.npy')
test_concept_labels = np.load(SHARED_DATA_PATH / 'test_concept_labels.npy')

# Generate citation labels (which evidence passages are relevant)
def generate_citation_labels(df, evidence_db):
    """Generate binary labels for evidence passage relevance"""
    citation_labels = []

    for _, row in df.iterrows():
        # Get diagnosis for this sample
        sample_diagnoses = [code for code in TARGET_CODES if row[code] == 1]

        # Mark relevant evidence passages (first MAX_EVIDENCE_PASSAGES)
        labels = []
        for i, evidence in enumerate(evidence_db[:MAX_EVIDENCE_PASSAGES]):
            # Evidence is relevant if it matches any diagnosis
            is_relevant = evidence['diagnosis'] in sample_diagnoses or evidence['diagnosis'] == 'general'
            labels.append(1 if is_relevant else 0)

        citation_labels.append(labels)

    return np.array(citation_labels)

# Generate action labels (recommended clinical actions based on diagnosis)
def generate_action_labels(df):
    """Generate binary labels for recommended clinical actions"""
    action_mapping = {
        'J189': [0, 2, 3, 9],  # Chest X-ray, O2, antibiotics, monitor
        'I5023': [4, 5, 9],    # Echo, diuretics, monitor
        'A419': [1, 2, 3, 8, 9],  # Cultures, O2, antibiotics, ICU, monitor
        'K8000': [6, 7, 9]     # Surgery, ultrasound, monitor
    }

    action_labels = []
    for _, row in df.iterrows():
        labels = [0] * len(CLINICAL_ACTIONS)
        for code in TARGET_CODES:
            if row[code] == 1:
                for action_idx in action_mapping[code]:
                    labels[action_idx] = 1
        action_labels.append(labels)

    return np.array(action_labels)

print("\nðŸ“Š Generating citation and action labels...")
train_citation_labels = generate_citation_labels(df_train, evidence_database)
val_citation_labels = generate_citation_labels(df_val, evidence_database)
test_citation_labels = generate_citation_labels(df_test, evidence_database)

train_action_labels = generate_action_labels(df_train)
val_action_labels = generate_action_labels(df_val)
test_action_labels = generate_action_labels(df_test)

print(f"âœ… Citation labels: {train_citation_labels.shape}")
print(f"âœ… Action labels: {train_action_labels.shape}")

# Dataset class
class RAGDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, concept_labels, citation_labels, action_labels):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.concept_labels = concept_labels
        self.citation_labels = citation_labels
        self.action_labels = action_labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            str(self.texts[idx]),
            truncation=True,
            max_length=512,
            padding='max_length',
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': torch.tensor(self.labels[idx], dtype=torch.float),
            'concept_labels': torch.tensor(self.concept_labels[idx], dtype=torch.float),
            'citation_labels': torch.tensor(self.citation_labels[idx], dtype=torch.float),
            'action_labels': torch.tensor(self.action_labels[idx], dtype=torch.float)
        }

# Create datasets
train_dataset = RAGDataset(df_train['text'].tolist(), df_train['labels'].tolist(), tokenizer,
                           train_concept_labels, train_citation_labels, train_action_labels)
val_dataset = RAGDataset(df_val['text'].tolist(), df_val['labels'].tolist(), tokenizer,
                         val_concept_labels, val_citation_labels, val_action_labels)
test_dataset = RAGDataset(df_test['text'].tolist(), df_test['labels'].tolist(), tokenizer,
                          test_concept_labels, test_citation_labels, test_action_labels)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)
test_loader = DataLoader(test_dataset, batch_size=16)

# Multi-objective loss
class MultiObjectiveLossPhase3(nn.Module):
    def __init__(self, lambda_dx, lambda_align, lambda_concept, lambda_cite, lambda_action):
        super().__init__()
        self.lambda_dx = lambda_dx
        self.lambda_align = lambda_align
        self.lambda_concept = lambda_concept
        self.lambda_cite = lambda_cite
        self.lambda_action = lambda_action
        self.bce = nn.BCEWithLogitsLoss()

    def forward(self, outputs, dx_labels, concept_labels, citation_labels, action_labels):
        # 1. Diagnosis loss
        loss_dx = self.bce(outputs['logits'], dx_labels)

        # 2. Alignment loss
        dx_probs = torch.sigmoid(outputs['logits'])
        concept_scores = outputs['concept_scores']
        loss_align = torch.abs(dx_probs.unsqueeze(-1) - concept_scores.unsqueeze(1)).mean()

        # 3. Concept prediction loss
        loss_concept = self.bce(outputs['concept_logits'], concept_labels)

        # 4. Citation loss (NEW)
        loss_cite = self.bce(outputs['citation_logits'], citation_labels)

        # 5. Action recommendation loss (NEW)
        loss_action = self.bce(outputs['action_logits'], action_labels)

        # Total loss
        total_loss = (
            self.lambda_dx * loss_dx +
            self.lambda_align * loss_align +
            self.lambda_concept * loss_concept +
            self.lambda_cite * loss_cite +
            self.lambda_action * loss_action
        )

        return total_loss, {
            'loss_dx': loss_dx.item(),
            'loss_align': loss_align.item(),
            'loss_concept': loss_concept.item(),
            'loss_cite': loss_cite.item(),
            'loss_action': loss_action.item(),
            'total_loss': total_loss.item()
        }

criterion = MultiObjectiveLossPhase3(LAMBDA_DX, LAMBDA_ALIGN, LAMBDA_CONCEPT, LAMBDA_CITE, LAMBDA_ACTION)
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
total_steps = len(train_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)

print("âœ… Training setup complete")

# ============================================================================
# TRAINING LOOP
# ============================================================================

print("\n" + "="*80)
print("ðŸ‹ï¸  TRAINING PHASE 3 (RAG + CITATION + ACTION)")
print("="*80)

best_val_f1 = 0.0
history = {'train_loss': [], 'val_loss': [], 'val_f1': [], 'val_cite_f1': [], 'val_action_f1': []}

concept_embeddings = concept_embedding_layer.weight.detach()

for epoch in range(EPOCHS):
    print(f"\nðŸ“ Epoch {epoch+1}/{EPOCHS}")

    # Training
    model.train()
    train_losses = []

    pbar = tqdm(train_loader, desc="Training")
    for batch in pbar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        concept_labels = batch['concept_labels'].to(device)
        citation_labels = batch['citation_labels'].to(device)
        action_labels = batch['action_labels'].to(device)

        optimizer.zero_grad()

        outputs = model(input_ids, attention_mask, concept_embeddings)
        loss, loss_components = criterion(outputs, labels, concept_labels, citation_labels, action_labels)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        train_losses.append(loss.item())
        pbar.set_postfix({'loss': f"{loss.item():.4f}"})

    avg_train_loss = np.mean(train_losses)
    history['train_loss'].append(avg_train_loss)

    # Validation
    model.eval()
    val_losses = []
    all_preds = []
    all_labels = []
    all_cite_preds = []
    all_cite_labels = []
    all_action_preds = []
    all_action_labels = []

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validation"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            concept_labels = batch['concept_labels'].to(device)
            citation_labels = batch['citation_labels'].to(device)
            action_labels = batch['action_labels'].to(device)

            outputs = model(input_ids, attention_mask, concept_embeddings)
            loss, _ = criterion(outputs, labels, concept_labels, citation_labels, action_labels)

            val_losses.append(loss.item())

            preds = (torch.sigmoid(outputs['logits']) > 0.5).cpu().numpy()
            cite_preds = (outputs['citation_scores'] > 0.5).cpu().numpy()
            action_preds = (outputs['action_scores'] > 0.5).cpu().numpy()

            all_preds.append(preds)
            all_labels.append(labels.cpu().numpy())
            all_cite_preds.append(cite_preds)
            all_cite_labels.append(citation_labels.cpu().numpy())
            all_action_preds.append(action_preds)
            all_action_labels.append(action_labels.cpu().numpy())

    all_preds = np.vstack(all_preds)
    all_labels = np.vstack(all_labels)
    all_cite_preds = np.vstack(all_cite_preds)
    all_cite_labels = np.vstack(all_cite_labels)
    all_action_preds = np.vstack(all_action_preds)
    all_action_labels = np.vstack(all_action_labels)

    avg_val_loss = np.mean(val_losses)
    val_f1 = f1_score(all_labels, all_preds, average='macro')
    val_cite_f1 = f1_score(all_cite_labels, all_cite_preds, average='macro', zero_division=0)
    val_action_f1 = f1_score(all_action_labels, all_action_preds, average='macro', zero_division=0)

    history['val_loss'].append(avg_val_loss)
    history['val_f1'].append(val_f1)
    history['val_cite_f1'].append(val_cite_f1)
    history['val_action_f1'].append(val_action_f1)

    print(f"   Train Loss:    {avg_train_loss:.4f}")
    print(f"   Val Loss:      {avg_val_loss:.4f}")
    print(f"   Val F1 (Dx):   {val_f1:.4f}")
    print(f"   Val F1 (Cite): {val_cite_f1:.4f}")
    print(f"   Val F1 (Act):  {val_action_f1:.4f}")

    # Save best model
    if val_f1 > best_val_f1:
        best_val_f1 = val_f1
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_f1': best_val_f1,
            'concept_embeddings': concept_embeddings,
            'evidence_database': evidence_database,
            'config': {
                'num_concepts': len(ALL_CONCEPTS),
                'num_diagnoses': len(TARGET_CODES),
                'num_actions': len(CLINICAL_ACTIONS),
                'max_evidence': MAX_EVIDENCE_PASSAGES
            }
        }, CHECKPOINT_PATH / 'phase3_v2_best.pt')
        print(f"   âœ… Saved best model (F1: {best_val_f1:.4f})")

# ============================================================================
# EVALUATION
# ============================================================================

print("\n" + "="*80)
print("ðŸ“Š FINAL EVALUATION")
print("="*80)

checkpoint = torch.load(CHECKPOINT_PATH / 'phase3_v2_best.pt', weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

all_preds = []
all_labels = []
all_cite_preds = []
all_cite_labels = []
all_action_preds = []
all_action_labels = []

with torch.no_grad():
    for batch in tqdm(test_loader, desc="Testing"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        citation_labels = batch['citation_labels'].to(device)
        action_labels = batch['action_labels'].to(device)

        outputs = model(input_ids, attention_mask, concept_embeddings)

        preds = (torch.sigmoid(outputs['logits']) > 0.5).cpu().numpy()
        cite_preds = (outputs['citation_scores'] > 0.5).cpu().numpy()
        action_preds = (outputs['action_scores'] > 0.5).cpu().numpy()

        all_preds.append(preds)
        all_labels.append(labels.cpu().numpy())
        all_cite_preds.append(cite_preds)
        all_cite_labels.append(citation_labels.cpu().numpy())
        all_action_preds.append(action_preds)
        all_action_labels.append(action_labels.cpu().numpy())

all_preds = np.vstack(all_preds)
all_labels = np.vstack(all_labels)
all_cite_preds = np.vstack(all_cite_preds)
all_cite_labels = np.vstack(all_cite_labels)
all_action_preds = np.vstack(all_action_preds)
all_action_labels = np.vstack(all_action_labels)

# Metrics
macro_f1 = f1_score(all_labels, all_preds, average='macro')
cite_f1 = f1_score(all_cite_labels, all_cite_preds, average='macro', zero_division=0)
action_f1 = f1_score(all_action_labels, all_action_preds, average='macro', zero_division=0)
per_class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)

print(f"\nðŸŽ¯ Diagnosis Performance:")
print(f"   Macro F1: {macro_f1:.4f}")

print(f"\nðŸ“Š Per-Class F1:")
for code, f1 in zip(TARGET_CODES, per_class_f1):
    print(f"   {code}: {f1:.4f} - {ICD_DESCRIPTIONS[code]}")

print(f"\nðŸ“š Citation Performance:")
print(f"   Citation F1: {cite_f1:.4f}")

print(f"\nðŸ’¡ Action Recommendation Performance:")
print(f"   Action F1: {action_f1:.4f}")

# Save results
results = {
    'phase': 'Phase 3 V2 - RAG + Citation Head + Action Head',
    'diagnosis_metrics': {
        'macro_f1': float(macro_f1),
        'per_class_f1': {code: float(f1) for code, f1 in zip(TARGET_CODES, per_class_f1)}
    },
    'citation_metrics': {
        'citation_f1': float(cite_f1)
    },
    'action_metrics': {
        'action_f1': float(action_f1)
    },
    'architecture': 'Concept Bottleneck + GraphSAGE + RAG + Citation Head + Action Head',
    'evidence_database_size': len(evidence_database),
    'training_history': history
}

with open(RESULTS_PATH / 'results.json', 'w') as f:
    json.dump(results, f, indent=2)

print(f"\nðŸ’¾ Results saved to: {RESULTS_PATH / 'results.json'}")
print(f"ðŸ’¾ Best model saved to: {CHECKPOINT_PATH / 'phase3_v2_best.pt'}")

print("\n" + "="*80)
print("âœ… PHASE 3 V2 COMPLETE!")
print("="*80)
print("\nKey Features:")
print("âœ… Evidence retrieval with dense retrieval (DPR)")
print("âœ… Citation Head for evidence attribution")
print("âœ… Action Head for clinical recommendations")
print("âœ… Multi-head outputs: Diagnosis + Citation + Action")
print("âœ… Evidence-grounded predictions")
print("\nNext: Phase 4 will add Uncertainty Quantification")
print("\nAlhamdulillah! ðŸ¤²")
