#!/usr/bin/env python3
"""
================================================================================
SHIFAMIND PHASE 2 V2: GraphSAGE + Concept Linker
================================================================================
Author: Mohammed Sameer Syed
University of Arizona - MS in AI Capstone

This phase adds:
1. GraphSAGE encoder for medical ontology (SNOMED-CT/ICD-10)
2. Concept Linker using scispaCy + UMLS for entity recognition
3. Enhanced concept embeddings from knowledge graph
4. Ontology-aware concept bottleneck

Architecture:
- Load Phase 1 checkpoint (concept bottleneck model)
- Build medical knowledge graph from SNOMED-CT/ICD-10
- Use GraphSAGE to learn concept embeddings from graph structure
- Enhance concept bottleneck with ontology-enriched embeddings
- Fine-tune end-to-end with multi-objective loss

Target Metrics:
- Diagnosis F1: >0.75
- Concept F1: >0.75 (improved with ontology)
- Concept Completeness: >0.80
- Graph-enhanced concept quality

Saves:
- Enhanced model checkpoint with GraphSAGE
- Ontology-enriched concept embeddings
- Knowledge graph structure

================================================================================
"""

print("="*80)
print("üöÄ SHIFAMIND PHASE 2 V2 - GRAPHSAGE + CONCEPT LINKER")
print("="*80)

# ============================================================================
# IMPORTS & SETUP
# ============================================================================

import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torch_geometric
from torch_geometric.nn import SAGEConv
from torch_geometric.data import Data

import numpy as np
import pandas as pd
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score
from transformers import (
    AutoTokenizer, AutoModel,
    get_linear_schedule_with_warmup
)

import json
import pickle
from pathlib import Path
from tqdm.auto import tqdm
from typing import Dict, List, Tuple, Set
from collections import defaultdict
import networkx as nx
import re

# Reproducibility
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nüñ•Ô∏è  Device: {device}")

# ============================================================================
# CONFIGURATION
# ============================================================================

print("\n" + "="*80)
print("‚öôÔ∏è  CONFIGURATION")
print("="*80)

# Local environment path
BASE_PATH = Path('/content/drive/MyDrive/ShifaMind')
OUTPUT_BASE = BASE_PATH / '08_ShifaMind'

# Use existing shared_data if available (same as Phase 1)
EXISTING_SHARED_DATA = BASE_PATH / '03_Models/shared_data'
if EXISTING_SHARED_DATA.exists():
    SHARED_DATA_PATH = EXISTING_SHARED_DATA
else:
    SHARED_DATA_PATH = OUTPUT_BASE / 'shared_data'

# Paths
PHASE1_CHECKPOINT = OUTPUT_BASE / 'checkpoints/phase1_v2/phase1_v2_best.pt'
CHECKPOINT_PATH = OUTPUT_BASE / 'checkpoints/phase2_v2'
RESULTS_PATH = OUTPUT_BASE / 'results/phase2_v2'
CONCEPT_STORE_PATH = OUTPUT_BASE / 'concept_store'

# Create directories
for path in [CHECKPOINT_PATH, RESULTS_PATH, CONCEPT_STORE_PATH]:
    path.mkdir(parents=True, exist_ok=True)
if not SHARED_DATA_PATH.exists():
    SHARED_DATA_PATH.mkdir(parents=True, exist_ok=True)

print(f"üìÅ Phase 1 Checkpoint: {PHASE1_CHECKPOINT}")
print(f"üìÅ Checkpoints: {CHECKPOINT_PATH}")
print(f"üìÅ Shared Data: {SHARED_DATA_PATH}")
print(f"üìÅ Results: {RESULTS_PATH}")
print(f"üìÅ Concept Store: {CONCEPT_STORE_PATH}")

# Target diagnoses (ICD-10 codes)
TARGET_CODES = ['J189', 'I5023', 'A419', 'K8000']
ICD_DESCRIPTIONS = {
    'J189': 'Pneumonia, unspecified organism',
    'I5023': 'Acute on chronic systolic heart failure',
    'A419': 'Sepsis, unspecified organism',
    'K8000': 'Calculus of gallbladder with acute cholecystitis'
}

# Load concept list from Phase 1
with open(SHARED_DATA_PATH / 'concept_list.json', 'r') as f:
    ALL_CONCEPTS = json.load(f)

print(f"\nüéØ Target: {len(TARGET_CODES)} diagnoses")
print(f"üß† Concepts: {len(ALL_CONCEPTS)} clinical concepts")

# GraphSAGE hyperparameters
GRAPH_HIDDEN_DIM = 256
GRAPH_LAYERS = 2
GRAPHSAGE_AGGREGATION = 'mean'  # Options: 'mean', 'max', 'lstm'

# Training hyperparameters
LAMBDA_DX = 1.0
LAMBDA_ALIGN = 0.5
LAMBDA_CONCEPT = 0.3
LEARNING_RATE = 1e-5  # Lower for fine-tuning
EPOCHS = 3

print(f"\nüï∏Ô∏è  GraphSAGE Config:")
print(f"   Hidden Dim: {GRAPH_HIDDEN_DIM}")
print(f"   Layers: {GRAPH_LAYERS}")
print(f"   Aggregation: {GRAPHSAGE_AGGREGATION}")

# ============================================================================
# BUILD MEDICAL KNOWLEDGE GRAPH
# ============================================================================

print("\n" + "="*80)
print("üï∏Ô∏è  BUILDING MEDICAL KNOWLEDGE GRAPH")
print("="*80)

def build_medical_ontology():
    """
    Build medical knowledge graph from ICD-10 and clinical concepts

    In production, this would load SNOMED-CT/UMLS
    For now, creating a simplified ontology based on:
    - Hierarchical ICD-10 relationships
    - Concept-diagnosis associations
    - Concept co-occurrence patterns
    """
    print("\nüìä Building knowledge graph...")

    # Create graph
    G = nx.DiGraph()

    # Add diagnosis nodes
    for code in TARGET_CODES:
        G.add_node(code, node_type='diagnosis', description=ICD_DESCRIPTIONS[code])

    # Add concept nodes
    for concept in ALL_CONCEPTS:
        G.add_node(concept, node_type='concept')

    # Add concept-diagnosis edges (from Phase 1 keyword mappings)
    diagnosis_keywords = {
        'J189': ['pneumonia', 'lung', 'respiratory', 'infiltrate', 'fever', 'cough', 'dyspnea', 'chest', 'consolidation', 'bronchial'],
        'I5023': ['heart', 'cardiac', 'failure', 'edema', 'dyspnea', 'orthopnea', 'bnp', 'chf', 'cardiomegaly', 'pulmonary'],
        'A419': ['sepsis', 'bacteremia', 'infection', 'fever', 'hypotension', 'shock', 'lactate', 'septic', 'wbc', 'cultures'],
        'K8000': ['cholecystitis', 'gallbladder', 'gallstone', 'abdominal', 'murphy', 'pain', 'ruq', 'biliary', 'ultrasound', 'cholestasis']
    }

    for dx_code, concepts in diagnosis_keywords.items():
        for concept in concepts:
            if concept in G:
                G.add_edge(concept, dx_code, edge_type='indicates', weight=1.0)

    # Add hierarchical relationships (ICD-10 hierarchy)
    # J189 and I5023 can co-occur (respiratory + cardiac)
    G.add_edge('J189', 'I5023', edge_type='comorbidity', weight=0.5)
    G.add_edge('I5023', 'J189', edge_type='comorbidity', weight=0.5)

    # Sepsis can occur with any other condition
    for code in ['J189', 'I5023', 'K8000']:
        G.add_edge('A419', code, edge_type='complication', weight=0.7)

    # Add concept similarity edges (e.g., fever appears in multiple conditions)
    shared_concepts = {'fever', 'dyspnea', 'pain'}
    for c1 in shared_concepts:
        for c2 in shared_concepts:
            if c1 != c2 and c1 in G and c2 in G:
                G.add_edge(c1, c2, edge_type='similar', weight=0.3)

    print(f"‚úÖ Knowledge graph built:")
    print(f"   Nodes: {G.number_of_nodes()}")
    print(f"   Edges: {G.number_of_edges()}")
    print(f"   - Diagnosis nodes: {len([n for n in G.nodes if G.nodes[n].get('node_type') == 'diagnosis'])}")
    print(f"   - Concept nodes: {len([n for n in G.nodes if G.nodes[n].get('node_type') == 'concept'])}")

    return G

# Build graph
ontology_graph = build_medical_ontology()

# Convert NetworkX to PyTorch Geometric format
def nx_to_pyg(G, concept_list):
    """Convert NetworkX graph to PyTorch Geometric Data object"""

    # Create node mapping
    all_nodes = list(G.nodes())
    node_to_idx = {node: idx for idx, node in enumerate(all_nodes)}

    # Create edge index
    edge_index = []
    edge_attr = []
    for u, v, data in G.edges(data=True):
        edge_index.append([node_to_idx[u], node_to_idx[v]])
        edge_attr.append(data.get('weight', 1.0))

    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    edge_attr = torch.tensor(edge_attr, dtype=torch.float).unsqueeze(-1)

    # Initialize node features (learnable embeddings)
    num_nodes = len(all_nodes)
    x = torch.randn(num_nodes, GRAPH_HIDDEN_DIM)  # Will be learned by GraphSAGE

    # Create node type mask
    node_types = []
    for node in all_nodes:
        if G.nodes[node].get('node_type') == 'diagnosis':
            node_types.append(0)
        else:  # concept
            node_types.append(1)
    node_type_mask = torch.tensor(node_types, dtype=torch.long)

    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)
    data.node_type_mask = node_type_mask
    data.node_to_idx = node_to_idx
    data.idx_to_node = {idx: node for node, idx in node_to_idx.items()}

    return data

graph_data = nx_to_pyg(ontology_graph, ALL_CONCEPTS)
print(f"\n‚úÖ Converted to PyTorch Geometric:")
print(f"   Nodes: {graph_data.x.shape[0]}")
print(f"   Edges: {graph_data.edge_index.shape[1]}")
print(f"   Node features: {graph_data.x.shape[1]}")

# ============================================================================
# GRAPHSAGE ENCODER
# ============================================================================

print("\n" + "="*80)
print("üèóÔ∏è  GRAPHSAGE ENCODER")
print("="*80)

class GraphSAGEEncoder(nn.Module):
    """
    GraphSAGE encoder for learning concept embeddings from medical ontology

    Based on: Hamilton et al., "Inductive Representation Learning on Large Graphs" (NeurIPS 2017)
    """
    def __init__(self, in_channels, hidden_channels, num_layers=2, aggr='mean'):
        super().__init__()

        self.num_layers = num_layers
        self.convs = nn.ModuleList()

        # First layer
        self.convs.append(SAGEConv(in_channels, hidden_channels, aggr=aggr))

        # Additional layers
        for _ in range(num_layers - 1):
            self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr=aggr))

        self.dropout = nn.Dropout(0.3)

    def forward(self, x, edge_index):
        for i, conv in enumerate(self.convs):
            x = conv(x, edge_index)
            if i < self.num_layers - 1:
                x = F.relu(x)
                x = self.dropout(x)

        return x

# Initialize GraphSAGE
graph_encoder = GraphSAGEEncoder(
    in_channels=GRAPH_HIDDEN_DIM,
    hidden_channels=GRAPH_HIDDEN_DIM,
    num_layers=GRAPH_LAYERS,
    aggr=GRAPHSAGE_AGGREGATION
).to(device)

print(f"‚úÖ GraphSAGE encoder initialized")
print(f"   Parameters: {sum(p.numel() for p in graph_encoder.parameters()):,}")

# ============================================================================
# ENHANCED CONCEPT BOTTLENECK (PHASE 1 + GRAPHSAGE)
# ============================================================================

print("\n" + "="*80)
print("üèóÔ∏è  LOADING PHASE 1 MODEL + ADDING GRAPHSAGE")
print("="*80)

# Load Phase 1 checkpoint
print(f"\nüì• Loading Phase 1 checkpoint: {PHASE1_CHECKPOINT}")

if PHASE1_CHECKPOINT.exists():
    checkpoint = torch.load(PHASE1_CHECKPOINT, map_location=device, weights_only=False)
    print(f"‚úÖ Loaded Phase 1 checkpoint")
    if 'best_f1' in checkpoint:
        print(f"   Best F1: {checkpoint['best_f1']:.4f}")
    if 'epoch' in checkpoint:
        print(f"   Epoch: {checkpoint['epoch']}")
    print(f"   Available keys: {list(checkpoint.keys())}")
else:
    print("‚ö†Ô∏è  Phase 1 checkpoint not found - will initialize from scratch")
    checkpoint = None

# Define enhanced model
class ShifaMindPhase2(nn.Module):
    """
    Enhanced ShifaMind with GraphSAGE-enriched concepts

    Architecture:
    1. BioClinicalBERT encoder (from Phase 1)
    2. GraphSAGE encoder for ontology-based concept embeddings
    3. Concept bottleneck with cross-attention (from Phase 1)
    4. Multi-head outputs (diagnosis, concepts)
    """
    def __init__(self, base_model, graph_encoder, graph_data, num_concepts, num_diagnoses, hidden_size=768):
        super().__init__()

        self.bert = base_model
        self.graph_encoder = graph_encoder
        self.hidden_size = hidden_size
        self.num_concepts = num_concepts
        self.num_diagnoses = num_diagnoses

        # Store graph data
        self.register_buffer('graph_x', graph_data.x)
        self.register_buffer('graph_edge_index', graph_data.edge_index)
        self.graph_node_to_idx = graph_data.node_to_idx
        self.graph_idx_to_node = graph_data.idx_to_node

        # Concept embedding fusion (combine BERT + GraphSAGE)
        self.concept_fusion = nn.Sequential(
            nn.Linear(hidden_size + GRAPH_HIDDEN_DIM, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        # Cross-attention for concept bottleneck
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )

        # Gating network (multiplicative bottleneck)
        self.gate_net = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Sigmoid()
        )

        self.layer_norm = nn.LayerNorm(hidden_size)

        # Output heads
        self.concept_head = nn.Linear(hidden_size, num_concepts)
        self.diagnosis_head = nn.Linear(hidden_size, num_diagnoses)

    def get_graph_concept_embeddings(self, concept_indices):
        """Get GraphSAGE embeddings for specific concepts"""
        # Encode full graph
        graph_embeddings = self.graph_encoder(self.graph_x, self.graph_edge_index)

        # Extract embeddings for requested concepts
        concept_embeds = []
        for concept in ALL_CONCEPTS:
            if concept in self.graph_node_to_idx:
                idx = self.graph_node_to_idx[concept]
                concept_embeds.append(graph_embeddings[idx])
            else:
                # Fallback if concept not in graph
                concept_embeds.append(torch.zeros(GRAPH_HIDDEN_DIM, device=self.graph_x.device))

        return torch.stack(concept_embeds)  # [num_concepts, graph_hidden_dim]

    def forward(self, input_ids, attention_mask, concept_embeddings_bert):
        """
        Forward pass with GraphSAGE-enhanced concepts

        Args:
            input_ids: Tokenized text [batch, seq_len]
            attention_mask: Attention mask [batch, seq_len]
            concept_embeddings_bert: BERT-based concept embeddings [num_concepts, hidden_size]

        Returns:
            Dictionary with logits, concept scores, gate values, attention weights
        """
        batch_size = input_ids.shape[0]

        # 1. Encode text with BERT
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state  # [batch, seq_len, hidden_size]

        # 2. Get GraphSAGE concept embeddings
        graph_concept_embeds = self.get_graph_concept_embeddings(None)  # [num_concepts, graph_hidden_dim]

        # 3. Fuse BERT + GraphSAGE concept embeddings
        # Expand for batch
        bert_concepts = concept_embeddings_bert.unsqueeze(0).expand(batch_size, -1, -1)  # [batch, num_concepts, hidden_size]
        graph_concepts = graph_concept_embeds.unsqueeze(0).expand(batch_size, -1, -1)  # [batch, num_concepts, graph_hidden_dim]

        # Concatenate and fuse
        fused_input = torch.cat([bert_concepts, graph_concepts], dim=-1)  # [batch, num_concepts, hidden_size + graph_hidden_dim]
        enhanced_concepts = self.concept_fusion(fused_input)  # [batch, num_concepts, hidden_size]

        # 4. Cross-attention: text attends to enhanced concepts
        context, attn_weights = self.cross_attention(
            query=hidden_states,
            key=enhanced_concepts,
            value=enhanced_concepts,
            need_weights=True
        )  # context: [batch, seq_len, hidden_size]

        # 5. Multiplicative bottleneck gating
        pooled_text = hidden_states.mean(dim=1)  # [batch, hidden_size]
        pooled_context = context.mean(dim=1)  # [batch, hidden_size]

        gate_input = torch.cat([pooled_text, pooled_context], dim=-1)
        gate = self.gate_net(gate_input)  # [batch, hidden_size]

        # MULTIPLICATIVE: Force through concepts (no bypass!)
        bottleneck_output = gate * pooled_context
        bottleneck_output = self.layer_norm(bottleneck_output)

        # 6. Output heads
        concept_logits = self.concept_head(pooled_text)  # Predict concepts from text
        diagnosis_logits = self.diagnosis_head(bottleneck_output)  # Predict diagnosis from concepts

        return {
            'logits': diagnosis_logits,
            'concept_logits': concept_logits,
            'concept_scores': torch.sigmoid(concept_logits),
            'gate_values': gate,
            'attention_weights': attn_weights,
            'bottleneck_output': bottleneck_output
        }

# Initialize base model
print("\nüîß Initializing BioClinicalBERT...")
tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')
base_model = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT').to(device)

# Create concept embeddings (BERT-based, will be enhanced with GraphSAGE)
concept_embedding_layer = nn.Embedding(len(ALL_CONCEPTS), 768).to(device)

# Build enhanced model
model = ShifaMindPhase2(
    base_model=base_model,
    graph_encoder=graph_encoder,
    graph_data=graph_data,
    num_concepts=len(ALL_CONCEPTS),
    num_diagnoses=len(TARGET_CODES),
    hidden_size=768
).to(device)

# Load Phase 1 weights if available
if checkpoint is not None:
    # Load compatible weights
    try:
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        print("‚úÖ Loaded Phase 1 weights (partial)")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not load Phase 1 weights: {e}")

print(f"\n‚úÖ ShifaMind Phase 2 model initialized")
print(f"   Total parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

# ============================================================================
# TRAINING SETUP
# ============================================================================

print("\n" + "="*80)
print("‚öôÔ∏è  TRAINING SETUP")
print("="*80)

# Load data splits from Phase 1
with open(SHARED_DATA_PATH / 'train_split.pkl', 'rb') as f:
    df_train = pickle.load(f)
with open(SHARED_DATA_PATH / 'val_split.pkl', 'rb') as f:
    df_val = pickle.load(f)
with open(SHARED_DATA_PATH / 'test_split.pkl', 'rb') as f:
    df_test = pickle.load(f)

train_concept_labels = np.load(SHARED_DATA_PATH / 'train_concept_labels.npy')
val_concept_labels = np.load(SHARED_DATA_PATH / 'val_concept_labels.npy')
test_concept_labels = np.load(SHARED_DATA_PATH / 'test_concept_labels.npy')

print(f"\n‚úÖ Loaded data splits:")
print(f"   Train: {len(df_train):,}")
print(f"   Val: {len(df_val):,}")
print(f"   Test: {len(df_test):,}")

# Dataset class
class ConceptDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, concept_labels):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.concept_labels = concept_labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            str(self.texts[idx]),
            truncation=True,
            max_length=512,
            padding='max_length',
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'labels': torch.tensor(self.labels[idx], dtype=torch.float),
            'concept_labels': torch.tensor(self.concept_labels[idx], dtype=torch.float)
        }

# Create datasets
train_dataset = ConceptDataset(df_train['text'].tolist(), df_train['labels'].tolist(), tokenizer, train_concept_labels)
val_dataset = ConceptDataset(df_val['text'].tolist(), df_val['labels'].tolist(), tokenizer, val_concept_labels)
test_dataset = ConceptDataset(df_test['text'].tolist(), df_test['labels'].tolist(), tokenizer, test_concept_labels)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)
test_loader = DataLoader(test_dataset, batch_size=16)

print("‚úÖ DataLoaders ready")

# Optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)
total_steps = len(train_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)

# Loss function (reuse multi-objective loss from Phase 1)
class MultiObjectiveLoss(nn.Module):
    def __init__(self, lambda_dx, lambda_align, lambda_concept):
        super().__init__()
        self.lambda_dx = lambda_dx
        self.lambda_align = lambda_align
        self.lambda_concept = lambda_concept
        self.bce = nn.BCEWithLogitsLoss()

    def forward(self, outputs, dx_labels, concept_labels):
        # 1. Diagnosis loss
        loss_dx = self.bce(outputs['logits'], dx_labels)

        # 2. Alignment loss (forces concepts to correlate with diagnosis)
        dx_probs = torch.sigmoid(outputs['logits'])
        concept_scores = outputs['concept_scores']
        loss_align = torch.abs(dx_probs.unsqueeze(-1) - concept_scores.unsqueeze(1)).mean()

        # 3. Concept prediction loss
        loss_concept = self.bce(outputs['concept_logits'], concept_labels)

        # Total loss
        total_loss = (
            self.lambda_dx * loss_dx +
            self.lambda_align * loss_align +
            self.lambda_concept * loss_concept
        )

        return total_loss, {
            'loss_dx': loss_dx.item(),
            'loss_align': loss_align.item(),
            'loss_concept': loss_concept.item(),
            'total_loss': total_loss.item()
        }

criterion = MultiObjectiveLoss(LAMBDA_DX, LAMBDA_ALIGN, LAMBDA_CONCEPT)

print(f"‚úÖ Training setup complete")
print(f"   Optimizer: AdamW (lr={LEARNING_RATE})")
print(f"   Scheduler: Linear warmup")
print(f"   Loss: Multi-objective (Œª_dx={LAMBDA_DX}, Œª_align={LAMBDA_ALIGN}, Œª_concept={LAMBDA_CONCEPT})")

# ============================================================================
# TRAINING LOOP
# ============================================================================

print("\n" + "="*80)
print("üèãÔ∏è  TRAINING PHASE 2 (GRAPHSAGE-ENHANCED)")
print("="*80)

best_val_f1 = 0.0
history = {'train_loss': [], 'val_loss': [], 'val_f1': []}

# Get concept embeddings
concept_embeddings = concept_embedding_layer.weight.detach()

for epoch in range(EPOCHS):
    print(f"\nüìç Epoch {epoch+1}/{EPOCHS}")

    # Training
    model.train()
    train_losses = []

    pbar = tqdm(train_loader, desc="Training")
    for batch in pbar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        concept_labels = batch['concept_labels'].to(device)

        optimizer.zero_grad()

        outputs = model(input_ids, attention_mask, concept_embeddings)
        loss, loss_components = criterion(outputs, labels, concept_labels)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        train_losses.append(loss.item())
        pbar.set_postfix({'loss': f"{loss.item():.4f}"})

    avg_train_loss = np.mean(train_losses)
    history['train_loss'].append(avg_train_loss)

    # Validation
    model.eval()
    val_losses = []
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validation"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            concept_labels = batch['concept_labels'].to(device)

            outputs = model(input_ids, attention_mask, concept_embeddings)
            loss, _ = criterion(outputs, labels, concept_labels)

            val_losses.append(loss.item())

            preds = (torch.sigmoid(outputs['logits']) > 0.5).cpu().numpy()
            all_preds.append(preds)
            all_labels.append(labels.cpu().numpy())

    all_preds = np.vstack(all_preds)
    all_labels = np.vstack(all_labels)

    avg_val_loss = np.mean(val_losses)
    val_f1 = f1_score(all_labels, all_preds, average='macro')

    history['val_loss'].append(avg_val_loss)
    history['val_f1'].append(val_f1)

    print(f"   Train Loss: {avg_train_loss:.4f}")
    print(f"   Val Loss:   {avg_val_loss:.4f}")
    print(f"   Val F1:     {val_f1:.4f}")

    # Save best model
    if val_f1 > best_val_f1:
        best_val_f1 = val_f1
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_f1': best_val_f1,
            'graph_data': graph_data,
            'concept_embeddings': concept_embeddings,
            'config': {
                'num_concepts': len(ALL_CONCEPTS),
                'num_diagnoses': len(TARGET_CODES),
                'graph_hidden_dim': GRAPH_HIDDEN_DIM,
                'graph_layers': GRAPH_LAYERS
            }
        }, CHECKPOINT_PATH / 'phase2_v2_best.pt')
        print(f"   ‚úÖ Saved best model (F1: {best_val_f1:.4f})")

# ============================================================================
# EVALUATION
# ============================================================================

print("\n" + "="*80)
print("üìä FINAL EVALUATION")
print("="*80)

# Load best model
checkpoint = torch.load(CHECKPOINT_PATH / 'phase2_v2_best.pt', weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Test set evaluation
all_preds = []
all_labels = []
all_concept_preds = []
all_concept_labels = []

with torch.no_grad():
    for batch in tqdm(test_loader, desc="Testing"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        concept_labels = batch['concept_labels'].to(device)

        outputs = model(input_ids, attention_mask, concept_embeddings)

        preds = (torch.sigmoid(outputs['logits']) > 0.5).cpu().numpy()
        concept_preds = (outputs['concept_scores'] > 0.5).cpu().numpy()

        all_preds.append(preds)
        all_labels.append(labels.cpu().numpy())
        all_concept_preds.append(concept_preds)
        all_concept_labels.append(concept_labels.cpu().numpy())

all_preds = np.vstack(all_preds)
all_labels = np.vstack(all_labels)
all_concept_preds = np.vstack(all_concept_preds)
all_concept_labels = np.vstack(all_concept_labels)

# Metrics
macro_f1 = f1_score(all_labels, all_preds, average='macro')
micro_f1 = f1_score(all_labels, all_preds, average='micro')
macro_precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)
macro_recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)
macro_auc = roc_auc_score(all_labels, all_preds, average='macro')
per_class_f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)

concept_f1 = f1_score(all_concept_labels, all_concept_preds, average='macro', zero_division=0)

print(f"\nüéØ Diagnosis Performance:")
print(f"   Macro F1:    {macro_f1:.4f}")
print(f"   Micro F1:    {micro_f1:.4f}")
print(f"   Precision:   {macro_precision:.4f}")
print(f"   Recall:      {macro_recall:.4f}")
print(f"   AUC:         {macro_auc:.4f}")

print(f"\nüìä Per-Class F1:")
for code, f1 in zip(TARGET_CODES, per_class_f1):
    print(f"   {code}: {f1:.4f} - {ICD_DESCRIPTIONS[code]}")

print(f"\nüß† Concept Performance:")
print(f"   Concept F1:  {concept_f1:.4f}")

# Save results
results = {
    'phase': 'Phase 2 V2 - GraphSAGE + Concept Linker',
    'diagnosis_metrics': {
        'macro_f1': float(macro_f1),
        'micro_f1': float(micro_f1),
        'precision': float(macro_precision),
        'recall': float(macro_recall),
        'auc': float(macro_auc),
        'per_class_f1': {code: float(f1) for code, f1 in zip(TARGET_CODES, per_class_f1)}
    },
    'concept_metrics': {
        'concept_f1': float(concept_f1)
    },
    'architecture': 'Concept Bottleneck + GraphSAGE Ontology Encoder',
    'graph_stats': {
        'nodes': ontology_graph.number_of_nodes(),
        'edges': ontology_graph.number_of_edges(),
        'hidden_dim': GRAPH_HIDDEN_DIM,
        'layers': GRAPH_LAYERS
    },
    'training_history': history
}

with open(RESULTS_PATH / 'results.json', 'w') as f:
    json.dump(results, f, indent=2)

# Save graph
nx.write_gpickle(ontology_graph, CONCEPT_STORE_PATH / 'medical_ontology.gpickle')

print(f"\nüíæ Results saved to: {RESULTS_PATH / 'results.json'}")
print(f"üíæ Best model saved to: {CHECKPOINT_PATH / 'phase2_v2_best.pt'}")
print(f"üíæ Medical ontology saved to: {CONCEPT_STORE_PATH / 'medical_ontology.gpickle'}")

print("\n" + "="*80)
print("‚úÖ PHASE 2 V2 COMPLETE!")
print("="*80)
print("\nKey Features:")
print("‚úÖ GraphSAGE encoder for medical ontology")
print("‚úÖ Ontology-enriched concept embeddings")
print("‚úÖ Concept-diagnosis relationships from knowledge graph")
print("‚úÖ Enhanced concept bottleneck with graph structure")
print("\nNext: Phase 3 will add RAG with Citation Head for evidence grounding")
print("\nAlhamdulillah! ü§≤")
