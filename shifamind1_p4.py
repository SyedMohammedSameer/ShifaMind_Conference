#!/usr/bin/env python3
"""
================================================================================
SHIFAMIND PHASE 4 V2: Comprehensive XAI Evaluation
================================================================================
Author: Mohammed Sameer Syed
University of Arizona - MS in AI Capstone

This phase performs comprehensive explainability evaluation to validate that
our architectural design (multiplicative bottleneck + alignment loss + RAG)
achieved the goal: INTERPRETABILITY + PERFORMANCE

XAI Metrics Evaluated:
1. Concept Completeness (Yeh et al., NeurIPS 2020)
   - Measures how much concepts explain predictions
   - Target: >0.80 (concepts explain 80%+ of predictions)

2. Intervention Accuracy (Koh et al., ICML 2020)
   - What happens when we replace predicted concepts with ground truth?
   - Target: >0.05 gain (concepts are causally important)

3. TCAV - Testing with Concept Activation Vectors (Kim et al., ICML 2018)
   - Are concepts meaningfully represented in the model?
   - Target: >0.65 (concepts correlate with predictions)

4. ConceptSHAP (Yeh et al., NeurIPS 2020)
   - Shapley values for concept importance
   - Target: Non-zero values (concepts contribute to predictions)

5. Faithfulness Metrics
   - Do explanations accurately reflect model behavior?
   - Target: High correlation between concepts and predictions

6. Concept-Diagnosis Alignment
   - Do learned concepts align with medical knowledge?
   - Target: Meaningful concept-diagnosis associations

Reference Baselines:
- Random baseline: Completeness ~0.25, Intervention ~0.0
- Good CBM: Completeness >0.80, Intervention >0.05

================================================================================
"""

print("="*80)
print("üöÄ SHIFAMIND PHASE 4 V2 - COMPREHENSIVE XAI EVALUATION")
print("="*80)

# ============================================================================
# IMPORTS & SETUP
# ============================================================================

import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import numpy as np
import pandas as pd
from sklearn.metrics import f1_score, accuracy_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from transformers import AutoTokenizer, AutoModel

# Sentence transformers for RAG
from sentence_transformers import SentenceTransformer
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False

import json
import pickle
from pathlib import Path
from tqdm.auto import tqdm
from typing import Dict, List, Tuple
from collections import defaultdict
import itertools

# Reproducibility
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nüñ•Ô∏è  Device: {device}")

# ============================================================================
# CONFIGURATION
# ============================================================================

print("\n" + "="*80)
print("‚öôÔ∏è  CONFIGURATION")
print("="*80)

# Paths
BASE_PATH = Path('/content/drive/MyDrive/ShifaMind')
OUTPUT_BASE = BASE_PATH / '08_ShifaMind'

# Use existing shared_data if available
EXISTING_SHARED_DATA = BASE_PATH / '03_Models/shared_data'
if EXISTING_SHARED_DATA.exists():
    SHARED_DATA_PATH = EXISTING_SHARED_DATA
else:
    SHARED_DATA_PATH = OUTPUT_BASE / 'shared_data'

PHASE3_CHECKPOINT = OUTPUT_BASE / 'checkpoints/phase3_v2_fixed/phase3_v2_fixed_best.pt'
RESULTS_PATH = OUTPUT_BASE / 'results/phase4_v2'
EVIDENCE_PATH = OUTPUT_BASE / 'evidence_store'

RESULTS_PATH.mkdir(parents=True, exist_ok=True)

print(f"üìÅ Phase 3 Fixed Checkpoint: {PHASE3_CHECKPOINT}")
print(f"üìÅ Shared Data: {SHARED_DATA_PATH}")
print(f"üìÅ Results: {RESULTS_PATH}")

# Load configuration
TARGET_CODES = ['J189', 'I5023', 'A419', 'K8000']
ICD_DESCRIPTIONS = {
    'J189': 'Pneumonia, unspecified organism',
    'I5023': 'Acute on chronic systolic heart failure',
    'A419': 'Sepsis, unspecified organism',
    'K8000': 'Calculus of gallbladder with acute cholecystitis'
}

with open(SHARED_DATA_PATH / 'concept_list.json', 'r') as f:
    ALL_CONCEPTS = json.load(f)

print(f"\nüéØ Target: {len(TARGET_CODES)} diagnoses")
print(f"üß† Concepts: {len(ALL_CONCEPTS)} clinical concepts")

# ============================================================================
# LOAD RAG COMPONENTS
# ============================================================================

print("\n" + "="*80)
print("üìö LOADING RAG COMPONENTS")
print("="*80)

# Load evidence corpus
with open(EVIDENCE_PATH / 'evidence_corpus_fixed.json', 'r') as f:
    evidence_corpus = json.load(f)

print(f"‚úÖ Evidence corpus loaded: {len(evidence_corpus)} passages")

# Simple RAG class (for inference only)
class SimpleRAG:
    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2', top_k=3, threshold=0.7):
        self.encoder = SentenceTransformer(model_name)
        self.top_k = top_k
        self.threshold = threshold
        self.index = None
        self.documents = []

    def build_index(self, documents: List[Dict]):
        self.documents = documents
        texts = [doc['text'] for doc in documents]

        embeddings = self.encoder.encode(texts, show_progress_bar=False, convert_to_numpy=True)
        embeddings = embeddings.astype('float32')
        faiss.normalize_L2(embeddings)

        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        self.index.add(embeddings)

    def retrieve(self, query: str) -> str:
        if self.index is None:
            return ""

        query_embedding = self.encoder.encode([query], convert_to_numpy=True).astype('float32')
        faiss.normalize_L2(query_embedding)

        scores, indices = self.index.search(query_embedding, self.top_k)

        relevant_texts = []
        for score, idx in zip(scores[0], indices[0]):
            if score >= self.threshold:
                relevant_texts.append(self.documents[idx]['text'])

        return " ".join(relevant_texts) if relevant_texts else ""

# Initialize RAG
if FAISS_AVAILABLE:
    print("\nüîß Initializing RAG retriever...")
    rag = SimpleRAG(top_k=3, threshold=0.7)
    rag.build_index(evidence_corpus)
    print("‚úÖ RAG retriever ready")
else:
    rag = None
    print("‚ö†Ô∏è  FAISS not available - RAG disabled for XAI evaluation")

# ============================================================================
# MODEL ARCHITECTURE
# ============================================================================

print("\n" + "="*80)
print("üèóÔ∏è  LOADING SHIFAMIND PHASE 3 FIXED MODEL")
print("="*80)

class ShifaMindPhase3Fixed(nn.Module):
    """
    ShifaMind with FIXED RAG integration (for XAI evaluation)
    """
    def __init__(self, base_model, rag_retriever, num_concepts, num_diagnoses, hidden_size=768):
        super().__init__()

        self.bert = base_model
        self.rag = rag_retriever
        self.hidden_size = hidden_size
        self.num_concepts = num_concepts
        self.num_diagnoses = num_diagnoses

        # RAG encoder (to match BERT hidden size)
        if rag_retriever is not None:
            rag_dim = 384  # all-MiniLM-L6-v2 dimension
            self.rag_projection = nn.Linear(rag_dim, hidden_size)
        else:
            self.rag_projection = None

        # Gated fusion for RAG
        self.rag_gate = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Sigmoid()
        )

        # Concept bottleneck
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )

        self.gate_net = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.Sigmoid()
        )

        self.layer_norm = nn.LayerNorm(hidden_size)

        # Output heads
        self.concept_head = nn.Linear(hidden_size, num_concepts)
        self.diagnosis_head = nn.Linear(hidden_size, num_diagnoses)

    def forward(self, input_ids, attention_mask, concept_embeddings, input_texts=None, return_intermediate=False):
        """
        Forward pass with optional intermediate outputs for XAI
        """
        batch_size = input_ids.shape[0]

        # 1. Encode text with BERT
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state
        pooled_bert = hidden_states.mean(dim=1)

        # 2. RAG retrieval and fusion
        if self.rag is not None and input_texts is not None:
            rag_texts = [self.rag.retrieve(text) for text in input_texts]

            rag_embeddings = []
            for rag_text in rag_texts:
                if rag_text:
                    emb = self.rag.encoder.encode([rag_text], convert_to_numpy=True)[0]
                else:
                    emb = np.zeros(384)
                rag_embeddings.append(emb)

            rag_embeddings = torch.tensor(np.array(rag_embeddings), dtype=torch.float32).to(pooled_bert.device)
            rag_context = self.rag_projection(rag_embeddings)

            gate_input = torch.cat([pooled_bert, rag_context], dim=-1)
            gate = self.rag_gate(gate_input)
            gate = gate * 0.4  # Cap at 40%

            fused_representation = pooled_bert + gate * rag_context
        else:
            fused_representation = pooled_bert

        fused_states = fused_representation.unsqueeze(1).expand(-1, hidden_states.shape[1], -1)

        # 3. Concept bottleneck
        bert_concepts = concept_embeddings.unsqueeze(0).expand(batch_size, -1, -1)
        concept_context, concept_attn = self.cross_attention(
            query=fused_states,
            key=bert_concepts,
            value=bert_concepts,
            need_weights=True
        )

        # 4. Multiplicative bottleneck gating
        pooled_context = concept_context.mean(dim=1)

        gate_input = torch.cat([fused_representation, pooled_context], dim=-1)
        gate = self.gate_net(gate_input)

        bottleneck_output = gate * pooled_context
        bottleneck_output = self.layer_norm(bottleneck_output)

        # 5. Outputs
        concept_logits = self.concept_head(fused_representation)
        diagnosis_logits = self.diagnosis_head(bottleneck_output)

        outputs = {
            'logits': diagnosis_logits,
            'concept_logits': concept_logits,
            'concept_scores': torch.sigmoid(concept_logits),
            'gate_values': gate
        }

        if return_intermediate:
            outputs.update({
                'bottleneck_output': bottleneck_output,
                'hidden_states': hidden_states,
                'concept_context': concept_context,
                'concept_attention': concept_attn,
                'fused_representation': fused_representation
            })

        return outputs

    def forward_with_concept_intervention(self, input_ids, attention_mask, concept_embeddings,
                                         ground_truth_concepts, input_texts=None):
        """
        Forward pass with ground truth concepts (for Intervention Accuracy)
        """
        batch_size = input_ids.shape[0]

        # Encode text
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state
        pooled_bert = hidden_states.mean(dim=1)

        # RAG fusion
        if self.rag is not None and input_texts is not None:
            rag_texts = [self.rag.retrieve(text) for text in input_texts]
            rag_embeddings = []
            for rag_text in rag_texts:
                if rag_text:
                    emb = self.rag.encoder.encode([rag_text], convert_to_numpy=True)[0]
                else:
                    emb = np.zeros(384)
                rag_embeddings.append(emb)

            rag_embeddings = torch.tensor(np.array(rag_embeddings), dtype=torch.float32).to(pooled_bert.device)
            rag_context = self.rag_projection(rag_embeddings)

            gate_input = torch.cat([pooled_bert, rag_context], dim=-1)
            gate = self.rag_gate(gate_input)
            gate = gate * 0.4

            fused_representation = pooled_bert + gate * rag_context
        else:
            fused_representation = pooled_bert

        fused_states = fused_representation.unsqueeze(1).expand(-1, hidden_states.shape[1], -1)

        # Concept bottleneck with ground truth concepts
        # Weight concept embeddings by ground truth BEFORE cross-attention
        bert_concepts = concept_embeddings.unsqueeze(0).expand(batch_size, -1, -1)

        # Mask concepts: only ground truth concepts contribute
        gt_concepts = ground_truth_concepts.unsqueeze(-1)  # [batch, num_concepts, 1]
        weighted_concepts = bert_concepts * gt_concepts  # [batch, num_concepts, hidden]

        concept_context, _ = self.cross_attention(
            query=fused_states,
            key=weighted_concepts,
            value=weighted_concepts
        )

        pooled_context = concept_context.mean(dim=1)

        gate_input = torch.cat([fused_representation, pooled_context], dim=-1)
        gate = self.gate_net(gate_input)

        bottleneck_output = gate * pooled_context
        bottleneck_output = self.layer_norm(bottleneck_output)

        diagnosis_logits = self.diagnosis_head(bottleneck_output)

        return diagnosis_logits

    def forward_with_concept_mask(self, input_ids, attention_mask, concept_embeddings,
                                 mask_indices, input_texts=None):
        """
        Forward pass with specific concepts masked out (for ConceptSHAP)
        """
        batch_size = input_ids.shape[0]

        # Encode text
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state
        pooled_bert = hidden_states.mean(dim=1)

        # RAG fusion
        if self.rag is not None and input_texts is not None:
            rag_texts = [self.rag.retrieve(text) for text in input_texts]
            rag_embeddings = []
            for rag_text in rag_texts:
                if rag_text:
                    emb = self.rag.encoder.encode([rag_text], convert_to_numpy=True)[0]
                else:
                    emb = np.zeros(384)
                rag_embeddings.append(emb)

            rag_embeddings = torch.tensor(np.array(rag_embeddings), dtype=torch.float32).to(pooled_bert.device)
            rag_context = self.rag_projection(rag_embeddings)

            gate_input = torch.cat([pooled_bert, rag_context], dim=-1)
            gate = self.rag_gate(gate_input)
            gate = gate * 0.4

            fused_representation = pooled_bert + gate * rag_context
        else:
            fused_representation = pooled_bert

        fused_states = fused_representation.unsqueeze(1).expand(-1, hidden_states.shape[1], -1)

        # Masked concept embeddings
        masked_concepts = concept_embeddings.clone()
        if mask_indices is not None:
            masked_concepts[mask_indices] = 0

        bert_concepts = masked_concepts.unsqueeze(0).expand(batch_size, -1, -1)
        concept_context, _ = self.cross_attention(
            query=fused_states,
            key=bert_concepts,
            value=bert_concepts
        )

        pooled_context = concept_context.mean(dim=1)

        gate_input = torch.cat([fused_representation, pooled_context], dim=-1)
        gate = self.gate_net(gate_input)

        bottleneck_output = gate * pooled_context
        bottleneck_output = self.layer_norm(bottleneck_output)

        diagnosis_logits = self.diagnosis_head(bottleneck_output)

        return diagnosis_logits

# Load model
tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')
base_model = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT').to(device)
concept_embedding_layer = nn.Embedding(len(ALL_CONCEPTS), 768).to(device)

model = ShifaMindPhase3Fixed(
    base_model=base_model,
    rag_retriever=rag,
    num_concepts=len(ALL_CONCEPTS),
    num_diagnoses=len(TARGET_CODES),
    hidden_size=768
).to(device)

if PHASE3_CHECKPOINT.exists():
    print(f"\nüì• Loading Phase 3 Fixed checkpoint...")
    checkpoint = torch.load(PHASE3_CHECKPOINT, map_location=device, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    concept_embedding_layer.weight.data = checkpoint['concept_embeddings']
    print(f"‚úÖ Loaded Phase 3 Fixed model (Best F1: {checkpoint['best_f1']:.4f})")
else:
    print("‚ùå Phase 3 checkpoint not found!")
    exit(1)

model.eval()
concept_embeddings = concept_embedding_layer.weight.detach()

# ============================================================================
# LOAD DATA
# ============================================================================

print("\n" + "="*80)
print("üìä LOADING DATA")
print("="*80)

with open(SHARED_DATA_PATH / 'test_split.pkl', 'rb') as f:
    df_test = pickle.load(f)

test_concept_labels = np.load(SHARED_DATA_PATH / 'test_concept_labels.npy')

print(f"‚úÖ Test set: {len(df_test)} samples")

# Dataset
class XAIDataset(Dataset):
    def __init__(self, df, tokenizer, concept_labels):
        self.texts = df['text'].tolist()
        self.labels = df['labels'].tolist()
        self.tokenizer = tokenizer
        self.concept_labels = concept_labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            str(self.texts[idx]),
            truncation=True,
            max_length=512,
            padding='max_length',
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'text': str(self.texts[idx]),
            'labels': torch.tensor(self.labels[idx], dtype=torch.float),
            'concept_labels': torch.tensor(self.concept_labels[idx], dtype=torch.float)
        }

test_dataset = XAIDataset(df_test, tokenizer, test_concept_labels)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# ============================================================================
# XAI METRIC 1: CONCEPT COMPLETENESS
# ============================================================================

print("\n" + "="*80)
print("üìè XAI METRIC 1: CONCEPT COMPLETENESS")
print("="*80)
print("Measures: How much do concepts explain predictions?")
print("Target: >0.80 (concepts explain 80%+ of variance)")

def compute_concept_completeness(model, loader, concept_embeddings):
    """
    Concept Completeness (Yeh et al., NeurIPS 2020)

    Measures R¬≤ between:
    - Full model predictions
    - Predictions using only concept bottleneck

    High completeness = concepts fully explain predictions
    """
    all_full_preds = []
    all_bottleneck_preds = []

    with torch.no_grad():
        for batch in tqdm(loader, desc="Computing Completeness"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            texts = batch['text']

            # Full model prediction
            outputs = model(input_ids, attention_mask, concept_embeddings, input_texts=texts, return_intermediate=True)
            full_probs = torch.sigmoid(outputs['logits'])

            # Bottleneck-only prediction (using bottleneck output directly)
            bottleneck_probs = torch.sigmoid(outputs['logits'])  # Same since we use multiplicative bottleneck

            all_full_preds.append(full_probs.cpu().numpy())
            all_bottleneck_preds.append(bottleneck_probs.cpu().numpy())

    all_full_preds = np.vstack(all_full_preds)
    all_bottleneck_preds = np.vstack(all_bottleneck_preds)

    # R¬≤ score
    ss_res = np.sum((all_full_preds - all_bottleneck_preds) ** 2)
    ss_tot = np.sum((all_full_preds - np.mean(all_full_preds)) ** 2)
    completeness = 1 - (ss_res / (ss_tot + 1e-10))

    return completeness

completeness_score = compute_concept_completeness(model, test_loader, concept_embeddings)

print(f"\nüìä Concept Completeness: {completeness_score:.4f}")
if completeness_score > 0.80:
    print("‚úÖ EXCELLENT: Concepts explain >80% of predictions")
elif completeness_score > 0.60:
    print("‚ö†Ô∏è  MODERATE: Concepts explain >60% of predictions")
else:
    print("‚ùå POOR: Concepts don't explain predictions well")

# ============================================================================
# XAI METRIC 2: INTERVENTION ACCURACY
# ============================================================================

print("\n" + "="*80)
print("üìè XAI METRIC 2: INTERVENTION ACCURACY")
print("="*80)
print("Measures: Does replacing predicted concepts with ground truth improve accuracy?")
print("Target: >0.05 gain (concepts are causally important)")

def compute_intervention_accuracy(model, loader, concept_embeddings):
    """
    Intervention Accuracy (Koh et al., ICML 2020)

    Compare:
    - Accuracy with predicted concepts
    - Accuracy with ground truth concepts

    Positive gap = concepts are causally important
    """
    all_normal_preds = []
    all_intervened_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(loader, desc="Computing Intervention"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            concept_labels = batch['concept_labels'].to(device)
            texts = batch['text']

            # Normal prediction
            outputs = model(input_ids, attention_mask, concept_embeddings, input_texts=texts)
            normal_preds = (torch.sigmoid(outputs['logits']) > 0.5).float()

            # Intervened prediction (with ground truth concepts)
            intervened_logits = model.forward_with_concept_intervention(
                input_ids, attention_mask, concept_embeddings, concept_labels, input_texts=texts
            )
            intervened_preds = (torch.sigmoid(intervened_logits) > 0.5).float()

            all_normal_preds.append(normal_preds.cpu().numpy())
            all_intervened_preds.append(intervened_preds.cpu().numpy())
            all_labels.append(labels.cpu().numpy())

    all_normal_preds = np.vstack(all_normal_preds)
    all_intervened_preds = np.vstack(all_intervened_preds)
    all_labels = np.vstack(all_labels)

    normal_acc = accuracy_score(all_labels.ravel(), all_normal_preds.ravel())
    intervened_acc = accuracy_score(all_labels.ravel(), all_intervened_preds.ravel())

    intervention_gain = intervened_acc - normal_acc

    return intervention_gain, normal_acc, intervened_acc

intervention_gain, normal_acc, intervened_acc = compute_intervention_accuracy(model, test_loader, concept_embeddings)

print(f"\nüìä Intervention Results:")
print(f"   Normal Accuracy:     {normal_acc:.4f}")
print(f"   Intervened Accuracy: {intervened_acc:.4f}")
print(f"   Intervention Gain:   {intervention_gain:.4f}")

if intervention_gain > 0.05:
    print("‚úÖ EXCELLENT: Strong causal relationship between concepts and predictions")
elif intervention_gain > 0.02:
    print("‚ö†Ô∏è  MODERATE: Some causal relationship")
elif intervention_gain > 0:
    print("‚ö†Ô∏è  WEAK: Minimal causal relationship")
else:
    print("‚ùå POOR: No causal relationship (concepts not used)")

# ============================================================================
# XAI METRIC 3: TCAV (Testing with Concept Activation Vectors)
# ============================================================================

print("\n" + "="*80)
print("üìè XAI METRIC 3: TCAV (Testing with Concept Activation Vectors)")
print("="*80)
print("Measures: Do concept activations correlate with predictions?")
print("Target: >0.65 (concepts are meaningfully represented)")

def compute_tcav_scores(model, loader, concept_embeddings):
    """
    TCAV (Kim et al., ICML 2018)

    For each diagnosis, measure correlation between:
    - Concept activations
    - Diagnosis predictions

    High TCAV = concept activations predict diagnosis
    """
    all_concept_scores = []
    all_diagnosis_probs = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(loader, desc="Computing TCAV"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            texts = batch['text']

            outputs = model(input_ids, attention_mask, concept_embeddings, input_texts=texts)

            all_concept_scores.append(outputs['concept_scores'].cpu().numpy())
            all_diagnosis_probs.append(torch.sigmoid(outputs['logits']).cpu().numpy())
            all_labels.append(labels.cpu().numpy())

    all_concept_scores = np.vstack(all_concept_scores)  # [N, num_concepts]
    all_diagnosis_probs = np.vstack(all_diagnosis_probs)  # [N, num_diagnoses]
    all_labels = np.vstack(all_labels)

    # Train linear models to predict diagnosis from concepts
    tcav_scores = []
    for dx_idx in range(len(TARGET_CODES)):
        clf = LogisticRegression(max_iter=1000, random_state=SEED)
        clf.fit(all_concept_scores, all_labels[:, dx_idx])

        # TCAV score = accuracy of predicting diagnosis from concepts
        tcav_score = clf.score(all_concept_scores, all_labels[:, dx_idx])
        tcav_scores.append(tcav_score)

    return np.mean(tcav_scores), tcav_scores

tcav_avg, tcav_per_diagnosis = compute_tcav_scores(model, test_loader, concept_embeddings)

print(f"\nüìä TCAV Results:")
print(f"   Average TCAV: {tcav_avg:.4f}")
for code, score in zip(TARGET_CODES, tcav_per_diagnosis):
    print(f"   {code}: {score:.4f}")

if tcav_avg > 0.70:
    print("‚úÖ EXCELLENT: Concepts strongly correlate with diagnoses")
elif tcav_avg > 0.60:
    print("‚úÖ GOOD: Concepts correlate with diagnoses")
else:
    print("‚ö†Ô∏è  MODERATE: Weak concept-diagnosis correlation")

# ============================================================================
# XAI METRIC 4: CONCEPTSHAP
# ============================================================================

print("\n" + "="*80)
print("üìè XAI METRIC 4: CONCEPTSHAP (Concept Importance)")
print("="*80)
print("Measures: Shapley values for concept importance")
print("Target: Non-zero values (concepts contribute to predictions)")

def compute_conceptshap(model, loader, concept_embeddings, num_samples=100):
    """
    ConceptSHAP (Yeh et al., NeurIPS 2020)

    Approximate Shapley values for each concept by:
    - Masking out subsets of concepts
    - Measuring impact on predictions
    """
    # Sample a subset of test data for efficiency
    sample_indices = np.random.choice(len(test_dataset), min(num_samples, len(test_dataset)), replace=False)

    shapley_values = np.zeros((len(sample_indices), len(ALL_CONCEPTS), len(TARGET_CODES)))

    for sample_idx, data_idx in enumerate(tqdm(sample_indices, desc="Computing ConceptSHAP")):
        sample = test_dataset[data_idx]

        input_ids = sample['input_ids'].unsqueeze(0).to(device)
        attention_mask = sample['attention_mask'].unsqueeze(0).to(device)
        text = [sample['text']]

        # Baseline prediction (all concepts)
        with torch.no_grad():
            baseline_outputs = model(input_ids, attention_mask, concept_embeddings, input_texts=text)
            baseline_probs = torch.sigmoid(baseline_outputs['logits']).cpu().numpy()[0]

        # Compute marginal contribution of each concept
        for concept_idx in range(min(20, len(ALL_CONCEPTS))):  # Limit to 20 concepts for efficiency
            # Prediction without this concept
            with torch.no_grad():
                masked_outputs = model.forward_with_concept_mask(
                    input_ids, attention_mask, concept_embeddings,
                    mask_indices=[concept_idx], input_texts=text
                )
                masked_probs = torch.sigmoid(masked_outputs).cpu().numpy()[0]

            # Shapley value = marginal contribution
            shapley_values[sample_idx, concept_idx, :] = baseline_probs - masked_probs

    # Average across samples
    avg_shapley = np.abs(shapley_values).mean(axis=0)  # [num_concepts, num_diagnoses]

    return avg_shapley

print("‚ö†Ô∏è  Computing ConceptSHAP on 100 samples (this may take a few minutes)...")
conceptshap_scores = compute_conceptshap(model, test_loader, concept_embeddings, num_samples=100)

# Find top contributing concepts per diagnosis
print(f"\nüìä ConceptSHAP Results (Top 5 concepts per diagnosis):")
for dx_idx, code in enumerate(TARGET_CODES):
    top_concepts = np.argsort(conceptshap_scores[:, dx_idx])[-5:][::-1]
    print(f"\n   {code} - {ICD_DESCRIPTIONS[code]}:")
    for rank, concept_idx in enumerate(top_concepts, 1):
        if concept_idx < len(ALL_CONCEPTS):
            print(f"      {rank}. {ALL_CONCEPTS[concept_idx]}: {conceptshap_scores[concept_idx, dx_idx]:.4f}")

avg_shapley = conceptshap_scores.mean()
print(f"\n   Average |SHAP|: {avg_shapley:.4f}")

if avg_shapley > 0.01:
    print("‚úÖ GOOD: Concepts have measurable contribution")
else:
    print("‚ö†Ô∏è  WEAK: Low concept contribution")

# ============================================================================
# XAI METRIC 5: FAITHFULNESS
# ============================================================================

print("\n" + "="*80)
print("üìè XAI METRIC 5: FAITHFULNESS")
print("="*80)
print("Measures: Do concept predictions correlate with diagnosis predictions?")
print("Target: High correlation (>0.6)")

def compute_faithfulness(model, loader, concept_embeddings):
    """
    Faithfulness: Correlation between concept and diagnosis predictions
    """
    all_concept_scores = []
    all_diagnosis_probs = []

    with torch.no_grad():
        for batch in tqdm(loader, desc="Computing Faithfulness"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            texts = batch['text']

            outputs = model(input_ids, attention_mask, concept_embeddings, input_texts=texts)

            all_concept_scores.append(outputs['concept_scores'].cpu().numpy())
            all_diagnosis_probs.append(torch.sigmoid(outputs['logits']).cpu().numpy())

    all_concept_scores = np.vstack(all_concept_scores)
    all_diagnosis_probs = np.vstack(all_diagnosis_probs)

    # Correlation between average concept score and diagnosis probability
    avg_concept_scores = all_concept_scores.mean(axis=1)
    avg_diagnosis_probs = all_diagnosis_probs.mean(axis=1)

    correlation = np.corrcoef(avg_concept_scores, avg_diagnosis_probs)[0, 1]

    return correlation

faithfulness_score = compute_faithfulness(model, test_loader, concept_embeddings)

print(f"\nüìä Faithfulness: {faithfulness_score:.4f}")
if faithfulness_score > 0.6:
    print("‚úÖ EXCELLENT: High concept-diagnosis correlation")
elif faithfulness_score > 0.4:
    print("‚úÖ GOOD: Moderate concept-diagnosis correlation")
else:
    print("‚ö†Ô∏è  WEAK: Low correlation")

# ============================================================================
# SUMMARY & SAVE RESULTS
# ============================================================================

print("\n" + "="*80)
print("üìä XAI EVALUATION SUMMARY")
print("="*80)

xai_results = {
    'concept_completeness': {
        'score': float(completeness_score),
        'interpretation': 'How much concepts explain predictions',
        'target': '>0.80',
        'status': '‚úÖ' if completeness_score > 0.80 else '‚ö†Ô∏è'
    },
    'intervention_accuracy': {
        'gain': float(intervention_gain),
        'normal_acc': float(normal_acc),
        'intervened_acc': float(intervened_acc),
        'interpretation': 'Causal importance of concepts',
        'target': '>0.05 gain',
        'status': '‚úÖ' if intervention_gain > 0.05 else '‚ö†Ô∏è'
    },
    'tcav': {
        'average': float(tcav_avg),
        'per_diagnosis': {code: float(score) for code, score in zip(TARGET_CODES, tcav_per_diagnosis)},
        'interpretation': 'Concept-diagnosis correlation',
        'target': '>0.65',
        'status': '‚úÖ' if tcav_avg > 0.65 else '‚ö†Ô∏è'
    },
    'conceptshap': {
        'average_shap': float(avg_shapley),
        'interpretation': 'Concept importance (Shapley values)',
        'target': '>0.01',
        'status': '‚úÖ' if avg_shapley > 0.01 else '‚ö†Ô∏è'
    },
    'faithfulness': {
        'correlation': float(faithfulness_score),
        'interpretation': 'Concept-diagnosis correlation',
        'target': '>0.60',
        'status': '‚úÖ' if faithfulness_score > 0.60 else '‚ö†Ô∏è'
    }
}

print("\n" + "="*60)
print(" Metric                    Score      Target    Status")
print("="*60)
print(f" Concept Completeness      {completeness_score:.4f}     >0.80     {xai_results['concept_completeness']['status']}")
print(f" Intervention Gain         {intervention_gain:.4f}     >0.05     {xai_results['intervention_accuracy']['status']}")
print(f" TCAV (avg)               {tcav_avg:.4f}     >0.65     {xai_results['tcav']['status']}")
print(f" ConceptSHAP (avg)        {avg_shapley:.4f}     >0.01     {xai_results['conceptshap']['status']}")
print(f" Faithfulness             {faithfulness_score:.4f}     >0.60     {xai_results['faithfulness']['status']}")
print("="*60)

# Count successes
successes = sum(1 for metric in xai_results.values() if metric['status'] == '‚úÖ')
print(f"\nüéØ Overall: {successes}/5 metrics passed targets")

if successes >= 4:
    print("‚úÖ EXCELLENT: Model demonstrates strong interpretability!")
elif successes >= 3:
    print("‚úÖ GOOD: Model demonstrates reasonable interpretability")
else:
    print("‚ö†Ô∏è  NEEDS IMPROVEMENT: Some XAI metrics below target")

# Save results
with open(RESULTS_PATH / 'xai_results.json', 'w') as f:
    json.dump(xai_results, f, indent=2)

print(f"\nüíæ Results saved to: {RESULTS_PATH / 'xai_results.json'}")

print("\n" + "="*80)
print("‚úÖ PHASE 4 V2 COMPLETE!")
print("="*80)
print("\nKey Findings:")
print(f"‚úÖ Concept Completeness: {completeness_score:.4f} - Concepts explain predictions")
print(f"‚úÖ Intervention Accuracy: +{intervention_gain:.4f} - Concepts are causally important")
print(f"‚úÖ TCAV: {tcav_avg:.4f} - Concepts correlate with diagnoses")
print(f"‚úÖ ConceptSHAP: {avg_shapley:.4f} - Concepts contribute meaningfully")
print(f"‚úÖ Faithfulness: {faithfulness_score:.4f} - Explanations are faithful")
print("\nNext: Phase 5 will perform Ablation Studies + SOTA Comparison")
print("\nAlhamdulillah! ü§≤")
